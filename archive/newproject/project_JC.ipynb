{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several works in the field of automatic tumor diagnosis can be divided into two main categories, namely feature extraction and sample classification. \n",
    "#### In general, the image properties are extracted first. These features usually include static features such as entropy, skewness, mean, energy, torque, and correlation or properties obtained by applying other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io, img_as_ubyte\n",
    "from kapur import kapur_threshold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io, img_as_ubyte\n",
    "from kapur import kapur_threshold\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Conv2D, Input, GlobalAveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend\n",
    "\n",
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(newdir, empty = True):\n",
    "    \"\"\"\n",
    "    create new folder if the target folder doesnt exist\n",
    "    \"\"\"\n",
    "    CHECK_FOLDER = os.path.isdir(newdir)\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(newdir)\n",
    "        print(\"created folder : \", newdir)\n",
    "\n",
    "    else:\n",
    "        if empty == True:\n",
    "            ## whether to remove all contents in the current augmented data folder and generate new ones\n",
    "            shutil.rmtree(newdir)\n",
    "            print(\"current augmented data removed\")\n",
    "            os.makedirs(newdir)\n",
    "        print(newdir, \"folder already exists.\")\n",
    "        \n",
    "## save the augmented data and the original ones in new folders        \n",
    "def data_augmentation(refresh=True, num=5):\n",
    "    \"\"\"\n",
    "    refresh: whether to replace current augmented data and generate new ones\n",
    "    num: number of augmented data per image\n",
    "    \"\"\"\n",
    "\n",
    "    training_path = \"data\\\\Training\"\n",
    "    ## destination parent folder for augmented data\n",
    "    augmented_path = \"data\\\\augmentation_training\"\n",
    "    current_directory = os.getcwd()\n",
    "    original_path = os.path.join(current_directory, training_path)\n",
    "    augmented_path = os.path.join(current_directory, augmented_path)\n",
    "\n",
    "    ## augmented data generator\n",
    "    image_generator = ImageDataGenerator(rotation_range=90, shear_range=0.4, zoom_range=0,\n",
    "                                         samplewise_center=True, vertical_flip=True, horizontal_flip=True,\n",
    "                                         samplewise_std_normalization=True)\n",
    "    for subf in os.listdir(original_path):\n",
    "        new_dir = os.path.join(augmented_path, subf)\n",
    "        create_dir(new_dir, empty=refresh)\n",
    "        for f in os.listdir(os.path.join(original_path, subf)):\n",
    "            image_path = os.path.join(original_path, subf, f)\n",
    "            img = load_img(image_path)\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)  # reshape to (1, height, width, channels)\n",
    "            i = 1\n",
    "            for batch in image_generator.flow(x, batch_size=1,\n",
    "                                              save_to_dir=new_dir,\n",
    "                                              save_prefix=f.split(\".\")[0],\n",
    "                                              save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > num:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder :  D:\\project\\data\\augmentation_training\\glioma\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mdata_augmentation\u001b[1;34m(refresh, num)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# reshape to (1, height, width, channels)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m image_generator\u001b[38;5;241m.\u001b[39mflow(x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     47\u001b[0m                                   save_to_dir\u001b[38;5;241m=\u001b[39mnew_dir,\n\u001b[0;32m     48\u001b[0m                                   save_prefix\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     49\u001b[0m                                   save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     50\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m num:\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:148\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 148\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:160\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m   index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:709\u001b[0m, in \u001b[0;36mNumpyArrayIterator._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    707\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[j]\n\u001b[0;32m    708\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mget_random_transform(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 709\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mstandardize(x)\n\u001b[0;32m    712\u001b[0m batch_x[i] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1800\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   1797\u001b[0m img_col_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1798\u001b[0m img_channel_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1800\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mapply_affine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtheta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_row_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_col_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_channel_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m   x \u001b[38;5;241m=\u001b[39m apply_channel_shift(x,\n\u001b[0;32m   1817\u001b[0m                           transform_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1818\u001b[0m                           img_channel_axis)\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:2324\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2321\u001b[0m final_affine_matrix \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2322\u001b[0m final_offset \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m-> 2324\u001b[0m channel_images \u001b[38;5;241m=\u001b[39m [ndimage\u001b[38;5;241m.\u001b[39minterpolation\u001b[38;5;241m.\u001b[39maffine_transform(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     x_channel,\n\u001b[0;32m   2326\u001b[0m     final_affine_matrix,\n\u001b[0;32m   2327\u001b[0m     final_offset,\n\u001b[0;32m   2328\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   2329\u001b[0m     mode\u001b[38;5;241m=\u001b[39mfill_mode,\n\u001b[0;32m   2330\u001b[0m     cval\u001b[38;5;241m=\u001b[39mcval) \u001b[38;5;28;01mfor\u001b[39;00m x_channel \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m   2331\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(channel_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2332\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(x, \u001b[38;5;241m0\u001b[39m, channel_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:2324\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2321\u001b[0m final_affine_matrix \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2322\u001b[0m final_offset \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m-> 2324\u001b[0m channel_images \u001b[38;5;241m=\u001b[39m [\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-complex-comprehension\u001b[39;49;00m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_affine_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_channel \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m   2331\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(channel_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2332\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(x, \u001b[38;5;241m0\u001b[39m, channel_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\applications\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:611\u001b[0m, in \u001b[0;36maffine_transform\u001b[1;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    608\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[0;32m    609\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cropped_image\n\u001b[0;32m     61\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchenj\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject-JieChen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeningioma\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTr-me_0011.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m new_img \u001b[38;5;241m=\u001b[39m \u001b[43mblur_and_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblur\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mblur_and_crop\u001b[1;34m(image, blur, cropping, kernel, masking, plot)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mpreprocessing:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m1. convert to grayscale and blur the image using median or gaussian filter\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m2. (optional)apply kapur thresholding to create a mask, mask the blurred image\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m3. crop the image to contain only the brain image, leaving the blank around surrounding the brain out.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert the image to grayscale, and blur it slightly\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blur \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     13\u001b[0m     blurred \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmedianBlur(gray, kernel)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "def blur_and_crop(image, blur = \"median\", cropping= False, kernel = 5, masking = True, plot=False):\n",
    "    \"\"\"\n",
    "    preprocessing:\n",
    "    1. convert to grayscale and blur the image using median or gaussian filter\n",
    "    2. (optional)apply kapur thresholding to create a mask, mask the blurred image\n",
    "    3. crop the image to contain only the brain image, leaving the blank around surrounding the brain out.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to grayscale, and blur it slightly\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if blur == \"median\":\n",
    "        blurred = cv2.medianBlur(gray, kernel)\n",
    "    elif blur == \"gaussian\":\n",
    "        blurred = cv2.GaussianBlur(gray, (kernel, kernel), 0)\n",
    "\n",
    "    # Threshold the image, then perform a series of erosions +\n",
    "    # dilations to remove any small regions of noise\n",
    "\n",
    "    if masking == True:\n",
    "   ## creating mask with kapur thresholding\n",
    "        threshold = kapur_threshold(blurred)\n",
    "        binr = cv2.threshold(blurred, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "        masked_image = cv2.bitwise_and(blurred, blurred, mask=binr)\n",
    "    else:\n",
    "        masked_image = blurred\n",
    "        \n",
    "    if cropping == True:\n",
    "        thresh = cv2.threshold(masked_image, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.erode(thresh, None, iterations=2)\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "        # Find contours in thresholded image, then grab the largest one\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "\n",
    "        # Find the extreme points for cropping\n",
    "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "        # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n",
    "        cropped_image = masked_image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]      \n",
    "    else:\n",
    "        cropped_image = masked_image\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(131), plt.imshow(image, cmap='gray'), plt.title('Original Image')\n",
    "        plt.subplot(132), plt.imshow(masked_image, cmap='gray'), plt.title('Masked Image')\n",
    "        plt.subplot(133), plt.imshow(cropped_image, cmap='gray'), plt.title('Cropped and Masked Image')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "img = cv2.imread(r\"C:\\Users\\chenj\\Desktop\\project-JieChen\\data\\Training\\meningioma\\Tr-me_0011.jpg\")\n",
    "new_img = blur_and_crop(img, blur = \"median\", cropping = False, kernel = 5, masking = True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\augmentation_Training'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_path = \"data\\\\Training\"\n",
    "\"\\\\augmentation_\".join(training_path.split(\"\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(training_path, masking=False, crop=False):\n",
    "    \"\"\"\n",
    "    preprocess the images in training_path parent folder\n",
    "    1. create a destination folder for preprocessed images\n",
    "    2. blur, mask and (crop) the images, masking is optional.\n",
    "    3. store the processed images in new folder\n",
    "    4. 提取图像特征并保存特征向量\n",
    "    \n",
    "    parameter: \n",
    "    training_path: the folder name for the original images to be processed\n",
    "    masking: if masking is applied in the processing\n",
    "    \"\"\"\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    ## destination parent folder for processed data\n",
    "    if masking == True:\n",
    "        processed_path = \"\\\\Processed_\".join(training_path.split(\"\\\\\"))\n",
    "    else:\n",
    "        processed_path = \"\\\\Unmasked_Processed_\".join(training_path.split(\"\\\\\"))\n",
    "    \n",
    "    processed_path = os.path.join(current_directory, processed_path)\n",
    "    original_path = os.path.join(current_directory, training_path)\n",
    "    \n",
    "    # 4. 提取图像特征并保存特征向量\n",
    "    feature_extractor = VGG16(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "    feature_extractor.trainable = False  # 冻结特征提取器的权重，只训练新加入的全连接层\n",
    "    \n",
    "    # 提取特征并保存特征向量\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    for subf in tqdm(os.listdir(original_path), desc=\"Folders\"):\n",
    "        new_dir = os.path.join(processed_path, subf)\n",
    "        create_dir(new_dir, empty=True)\n",
    "\n",
    "        for f in tqdm(os.listdir(os.path.join(original_path, subf)), desc=\"Images\"):\n",
    "            image_path = os.path.join(original_path, subf, f)\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # VGG16使用RGB格式\n",
    "\n",
    "            # 保存预处理后的图像\n",
    "            new_img = blur_and_crop(img, blur=\"median\", cropping=crop, kernel=5, masking=masking, plot=False)\n",
    "            image = Image.fromarray(new_img)\n",
    "            image.save(os.path.join(new_dir, f))\n",
    "\n",
    "            # 提取特征\n",
    "            features = feature_extractor.predict(np.expand_dims(img, axis=0))\n",
    "            features_list.append(features.flatten())  # 将特征展平为向量\n",
    "            labels_list.append(subf)  # 添加标签\n",
    "\n",
    "    # 保存特征向量为Numpy数组\n",
    "    features_array = np.array(features_list)\n",
    "    np.save('features.npy', features_array)\n",
    "    # 保存标签为Numpy数组\n",
    "    labels_array = np.array(labels_list)\n",
    "    np.save('labels.npy', labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\jiazh\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/jiazh/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# preprocessing(training_path=\"data\\\\Training\", masking=False)\n",
    "# preprocessing(training_path=\"data\\\\Training\", masking=True)\n",
    "# # 这里python会崩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = \"data\\\\Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179/179 [==============================] - 75s 372ms/step - loss: 3.7055 - accuracy: 0.7169\n",
      "Epoch 2/10\n",
      "179/179 [==============================] - 65s 363ms/step - loss: 0.9305 - accuracy: 0.7948\n",
      "Epoch 3/10\n",
      "179/179 [==============================] - 66s 371ms/step - loss: 0.6487 - accuracy: 0.8424\n",
      "Epoch 4/10\n",
      "179/179 [==============================] - 67s 372ms/step - loss: 0.5788 - accuracy: 0.8591\n",
      "Epoch 5/10\n",
      "179/179 [==============================] - 63s 350ms/step - loss: 0.4514 - accuracy: 0.8909\n",
      "Epoch 6/10\n",
      "179/179 [==============================] - 62s 348ms/step - loss: 0.3704 - accuracy: 0.9055\n",
      "Epoch 7/10\n",
      "179/179 [==============================] - 65s 363ms/step - loss: 0.3427 - accuracy: 0.9081\n",
      "Epoch 8/10\n",
      "179/179 [==============================] - 63s 350ms/step - loss: 0.2951 - accuracy: 0.9251\n",
      "Epoch 9/10\n",
      "179/179 [==============================] - 62s 346ms/step - loss: 0.3249 - accuracy: 0.9270\n",
      "Epoch 10/10\n",
      "179/179 [==============================] - 64s 360ms/step - loss: 0.2323 - accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    # 加载预处理后的特征向量和标签\n",
    "    features = np.load('features.npy')\n",
    "    labels = np.load('labels.npy')\n",
    "    return features, labels\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    # 构建模型\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def train_model(features, labels):\n",
    "    # 标签编码为数字\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # 进行独热编码\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    labels_onehot = onehot_encoder.fit_transform(labels_encoded.reshape(-1, 1))\n",
    "    \n",
    "    # 设置输入形状和类别数量\n",
    "    input_shape = features.shape[1:]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # 构建模型\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    # 编译模型\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(features, labels_onehot, batch_size=32, epochs=10)\n",
    "    \n",
    "    return model\n",
    "if __name__ == '__main__':\n",
    "    features, labels = load_data(\"data\\\\Training\")\n",
    "    \n",
    "    # 训练模型\n",
    "    model = train_model(features, labels)\n",
    "\n",
    "    # 加载测试集数据\n",
    "    testing_path = \"data\\\\Testing\"\n",
    "    testing_features, testing_labels = load_data(testing_path)\n",
    "    \n",
    "    # 在测试集进行评估\n",
    "    testing_labels_encoded = LabelEncoder().fit_transform(testing_labels)\n",
    "    testing_labels_onehot = OneHotEncoder(sparse=False).fit_transform(testing_labels_encoded.reshape(-1, 1))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(testing_features, testing_labels_onehot, batch_size=32)\n",
    "    print(f\"Testing Loss: {loss}, Testing Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea143e4bc99e98eae9999230d26dbec0476867e2e441c4be191545fe76cb5bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
